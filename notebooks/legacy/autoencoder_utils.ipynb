{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GAE(torch.nn.Module):\n",
    "    def __init__(self, encoder: Module, decoder: Optional[Module] = None):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = InnerProductDecoder() if decoder is None else decoder\n",
    "        GAE.reset_parameters(self)\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        reset(self.encoder)\n",
    "        reset(self.decoder)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, *args, **kwargs) -> Tensor:  # pragma: no cover\n",
    "        r\"\"\"Alias for :meth:`encode`.\"\"\"\n",
    "        return self.encoder(*args, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "    def encode(self, *args, **kwargs) -> Tensor:\n",
    "        r\"\"\"Runs the encoder and computes node-wise latent variables.\"\"\"\n",
    "        return self.encoder(*args, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "    def decode(self, *args, **kwargs) -> Tensor:\n",
    "        r\"\"\"Runs the decoder and computes edge probabilities.\"\"\"\n",
    "        return self.decoder(*args, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "    def recon_loss(self, z: Tensor, pos_edge_index: Tensor,\n",
    "                    neg_edge_index: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Given latent variables :obj:`z`, computes the binary cross\n",
    "        entropy loss for positive edges :obj:`pos_edge_index` and negative\n",
    "        sampled edges.\n",
    "\n",
    "        Args:\n",
    "            z (torch.Tensor): The latent space :math:`\\mathbf{Z}`.\n",
    "            pos_edge_index (torch.Tensor): The positive edges to train against.\n",
    "            neg_edge_index (torch.Tensor, optional): The negative edges to\n",
    "                train against. If not given, uses negative sampling to\n",
    "                calculate negative edges. (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        pos_loss = -torch.log(\n",
    "            self.decoder(z, pos_edge_index, sigmoid=True) + EPS).mean()\n",
    "\n",
    "        if neg_edge_index is None:\n",
    "            neg_edge_index = negative_sampling(pos_edge_index, z.size(0))\n",
    "        neg_loss = -torch.log(1 -\n",
    "                                self.decoder(z, neg_edge_index, sigmoid=True) +\n",
    "                                EPS).mean()\n",
    "\n",
    "        return pos_loss + neg_loss\n",
    "\n",
    "\n",
    "\n",
    "    def test(self, z: Tensor, pos_edge_index: Tensor,\n",
    "                neg_edge_index: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        r\"\"\"Given latent variables :obj:`z`, positive edges\n",
    "        :obj:`pos_edge_index` and negative edges :obj:`neg_edge_index`,\n",
    "        computes area under the ROC curve (AUC) and average precision (AP)\n",
    "        scores.\n",
    "\n",
    "        Args:\n",
    "            z (torch.Tensor): The latent space :math:`\\mathbf{Z}`.\n",
    "            pos_edge_index (torch.Tensor): The positive edges to evaluate\n",
    "                against.\n",
    "            neg_edge_index (torch.Tensor): The negative edges to evaluate\n",
    "                against.\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "        pos_y = z.new_ones(pos_edge_index.size(1))\n",
    "        neg_y = z.new_zeros(neg_edge_index.size(1))\n",
    "        y = torch.cat([pos_y, neg_y], dim=0)\n",
    "\n",
    "        pos_pred = self.decoder(z, pos_edge_index, sigmoid=True)\n",
    "        neg_pred = self.decoder(z, neg_edge_index, sigmoid=True)\n",
    "        pred = torch.cat([pos_pred, neg_pred], dim=0)\n",
    "\n",
    "        y, pred = y.detach().cpu().numpy(), pred.detach().cpu().numpy()\n",
    "\n",
    "        return roc_auc_score(y, pred), average_precision_score(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGAE(GAE):\n",
    "    def __init__(self, encoder: Module, decoder: Optional[Module] = None):\n",
    "        super().__init__(encoder, decoder)\n",
    "\n",
    "    def reparametrize(self, mu: Tensor, logstd: Tensor) -> Tensor:\n",
    "        if self.training:\n",
    "            return mu + torch.randn_like(logstd) * torch.exp(logstd)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def encode(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        self.__mu__, self.__logstd__ = self.encoder(*args, **kwargs)\n",
    "        self.__logstd__ = self.__logstd__.clamp(max=MAX_LOGSTD)\n",
    "        z = self.reparametrize(self.__mu__, self.__logstd__)\n",
    "        return z\n",
    "\n",
    "\n",
    "    def kl_loss(self, mu: Optional[Tensor] = None,\n",
    "                logstd: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Computes the KL loss, either for the passed arguments :obj:`mu`\n",
    "        and :obj:`logstd`, or based on latent variables from last encoding.\n",
    "\n",
    "        Args:\n",
    "            mu (torch.Tensor, optional): The latent space for :math:`\\mu`. If\n",
    "                set to :obj:`None`, uses the last computation of :math:`\\mu`.\n",
    "                (default: :obj:`None`)\n",
    "            logstd (torch.Tensor, optional): The latent space for\n",
    "                :math:`\\log\\sigma`.  If set to :obj:`None`, uses the last\n",
    "                computation of :math:`\\log\\sigma^2`. (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        mu = self.__mu__ if mu is None else mu\n",
    "        logstd = self.__logstd__ if logstd is None else logstd.clamp(\n",
    "            max=MAX_LOGSTD)\n",
    "        return -0.5 * torch.mean(\n",
    "            torch.sum(1 + 2 * logstd - mu**2 - logstd.exp()**2, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GAE, VGAE, GCNConv\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--variational', action='store_true')\n",
    "parser.add_argument('--linear', action='store_true')\n",
    "parser.add_argument('--dataset', type=str, default='Cora',\n",
    "                    choices=['Cora', 'CiteSeer', 'PubMed'])\n",
    "parser.add_argument('--epochs', type=int, default=400)\n",
    "args = parser.parse_args()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      split_labels=True, add_negative_train_samples=False),\n",
    "])\n",
    "path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Planetoid')\n",
    "dataset = Planetoid(path, args.dataset, transform=transform)\n",
    "train_data, val_data, test_data = dataset[0]\n",
    "\n",
    "\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        self.conv2 = GCNConv(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "\n",
    "class VariationalGCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        self.conv_mu = GCNConv(2 * out_channels, out_channels)\n",
    "        self.conv_logstd = GCNConv(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n",
    "\n",
    "\n",
    "class LinearEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = GCNConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.conv(x, edge_index)\n",
    "\n",
    "\n",
    "class VariationalLinearEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_mu = GCNConv(in_channels, out_channels)\n",
    "        self.conv_logstd = GCNConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n",
    "\n",
    "\n",
    "in_channels, out_channels = dataset.num_features, 16\n",
    "\n",
    "if not args.variational and not args.linear:\n",
    "    model = GAE(GCNEncoder(in_channels, out_channels))\n",
    "elif not args.variational and args.linear:\n",
    "    model = GAE(LinearEncoder(in_channels, out_channels))\n",
    "elif args.variational and not args.linear:\n",
    "    model = VGAE(VariationalGCNEncoder(in_channels, out_channels))\n",
    "elif args.variational and args.linear:\n",
    "    model = VGAE(VariationalLinearEncoder(in_channels, out_channels))\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "    loss = model.recon_loss(z, train_data.pos_edge_label_index)\n",
    "    if args.variational:\n",
    "        loss = loss + (1 / train_data.num_nodes) * model.kl_loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    return model.test(z, data.pos_edge_label_index, data.neg_edge_label_index)\n",
    "\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    loss = train()\n",
    "    auc, ap = test(test_data)\n",
    "    print(f'Epoch: {epoch:03d}, AUC: {auc:.4f}, AP: {ap:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
